name: 36Kr News Crawler
on:
  schedule:
    # Run 4 times per day: at 1:00, 7:00, 13:00, and 19:00 UTC
    # (corresponding to 9:00, 15:00, 21:00, and 3:00 Beijing time)
    - cron: '0 1,7,13,19 * * *'
  # Also allow manual triggering of workflow
  workflow_dispatch:
    inputs:
      days_ago:
        description: '获取几天前的新闻(0=今天，1=昨天)'
        required: true
        default: '0'
        type: choice
        options:
          - '0'
          - '1'
          - '2'
          - '3'
          - '7'
      max_pages:
        description: '最多获取页数'
        required: true
        default: '5'
        type: string
      page_size:
        description: '每页新闻数量'
        required: true
        default: '100'
        type: string
# Add permission configuration
permissions:
  contents: write
jobs:
  crawl-36kr:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
          
      - name: Run crawler (scheduled)
        if: github.event_name == 'schedule'
        run: |
          # Create data directory
          mkdir -p ./data
          # Crawl yesterday's news (to ensure a full day's news has been published)
          python 36kr.py --days 1 --pages 10 --size 100 --dir ./data
          
      - name: Run crawler (manual)
        if: github.event_name == 'workflow_dispatch'
        run: |
          # Create data directory
          mkdir -p ./data
          # Use user-provided parameters
          python 36kr.py --days ${{ github.event.inputs.days_ago }} --pages ${{ github.event.inputs.max_pages }} --size ${{ github.event.inputs.page_size }} --dir ./data
          
      - name: Push to data branch
        # Push results to dedicated data branch
        run: |
          # Configure Git
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          
          # Create timestamp directory to prevent conflicts
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          mkdir -p ./archive/$TIMESTAMP
          cp -r ./data/* ./archive/$TIMESTAMP/
          
          # Add README if it doesn't exist
          if [ ! -f README.md ]; then
            echo "# 36氪新闻数据" > README.md
            echo "此仓库通过GitHub Actions自动抓取36氪网站的最新新闻。" >> README.md
            echo "数据每天自动更新4次 (北京时间9点, 15点, 21点, 和次日3点)。" >> README.md
          fi
          
          # Add all changes
          git add ./data ./archive README.md
          
          # Commit changes
          git commit -m "Update 36kr news data: $TIMESTAMP" || echo "No changes to commit"
          
          # Push to original branch
          git push origin HEAD || (git pull --rebase && git push origin HEAD)
          
      - name: Archive crawling results
        uses: actions/upload-artifact@v4
        with:
          name: 36kr-news-data
          path: ./data/
          retention-days: 7
